{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning (with Keras)\n",
    "\n",
    "In this assignment we will be exploring deep learnign with Keras and the Fasion-MNIST dataset. \n",
    "\n",
    "We start with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We now load and organise our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, test_size = 0.16666, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model\n",
    "\n",
    "We'll now build our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_net = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_net.add(layers.Dense(512, activation='relu', input_shape= (28 * 28,)))\n",
    "initial_net.add(layers.Dense(512, activation='relu'))\n",
    "initial_net.add(layers.Dense(512, activation='relu'))\n",
    "initial_net.add(layers.Dense(512, activation='relu'))\n",
    "initial_net.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_net.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.5106 - acc: 0.8377 - val_loss: 0.2187 - val_acc: 0.9347\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.1562 - acc: 0.9531 - val_loss: 0.2447 - val_acc: 0.9312\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0980 - acc: 0.9704 - val_loss: 0.1114 - val_acc: 0.9676\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 8s 163us/step - loss: 0.0654 - acc: 0.9802 - val_loss: 0.1104 - val_acc: 0.9703\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0493 - acc: 0.9851 - val_loss: 0.0971 - val_acc: 0.9740\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.1158 - val_acc: 0.9735\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.0276 - acc: 0.9916 - val_loss: 0.0967 - val_acc: 0.9769\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.0250 - acc: 0.9928 - val_loss: 0.1046 - val_acc: 0.9765\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.0197 - acc: 0.9938 - val_loss: 0.1333 - val_acc: 0.9731\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1449 - val_acc: 0.9700\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.1243 - val_acc: 0.9746\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 6s 117us/step - loss: 0.0152 - acc: 0.9960 - val_loss: 0.1257 - val_acc: 0.9774\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.0135 - acc: 0.9962 - val_loss: 0.1173 - val_acc: 0.9797\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.0134 - acc: 0.9963 - val_loss: 0.1073 - val_acc: 0.9808\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.0113 - acc: 0.9968 - val_loss: 0.1540 - val_acc: 0.9756\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.0123 - acc: 0.9967 - val_loss: 0.1278 - val_acc: 0.9777\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.1791 - val_acc: 0.9743\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.0108 - acc: 0.9973 - val_loss: 0.1415 - val_acc: 0.9770\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.0073 - acc: 0.9981 - val_loss: 0.1294 - val_acc: 0.9809\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.1607 - val_acc: 0.9766\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.0087 - acc: 0.9978 - val_loss: 0.1255 - val_acc: 0.9834\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.0100 - acc: 0.9977 - val_loss: 0.1448 - val_acc: 0.9790\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.0085 - acc: 0.9981 - val_loss: 0.1915 - val_acc: 0.9754\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.0107 - acc: 0.9974 - val_loss: 0.1287 - val_acc: 0.9803\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1540 - val_acc: 0.9798\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.0062 - acc: 0.9985 - val_loss: 0.1517 - val_acc: 0.9803\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.1566 - val_acc: 0.9799\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.0084 - acc: 0.9981 - val_loss: 0.1739 - val_acc: 0.9785\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.1458 - val_acc: 0.9810\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.0077 - acc: 0.9985 - val_loss: 0.1255 - val_acc: 0.9829\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.1573 - val_acc: 0.9801\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.1428 - val_acc: 0.9804\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 0.0065 - acc: 0.9983 - val_loss: 0.1370 - val_acc: 0.9815\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0070 - acc: 0.9987 - val_loss: 0.1369 - val_acc: 0.9836\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.1535 - val_acc: 0.9812\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.1485 - val_acc: 0.9819\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.1717 - val_acc: 0.9787\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0081 - acc: 0.9984 - val_loss: 0.1521 - val_acc: 0.9820\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.1408 - val_acc: 0.9829\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.1503 - val_acc: 0.9830\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0077 - acc: 0.9988 - val_loss: 0.1642 - val_acc: 0.9806\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.1760 - val_acc: 0.9821\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.0090 - acc: 0.9987 - val_loss: 0.1777 - val_acc: 0.9788\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1509 - val_acc: 0.9822\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1771 - val_acc: 0.9815\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0072 - acc: 0.9987 - val_loss: 0.1927 - val_acc: 0.9810\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.1757 - val_acc: 0.9800\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1780 - val_acc: 0.9803\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.1701 - val_acc: 0.9816\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.1719 - val_acc: 0.9826\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0087 - acc: 0.9986 - val_loss: 0.1785 - val_acc: 0.9799\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.1716 - val_acc: 0.9810\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0062 - acc: 0.9989 - val_loss: 0.1639 - val_acc: 0.9826\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.0070 - acc: 0.9987 - val_loss: 0.1427 - val_acc: 0.9842\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.1987 - val_acc: 0.9806\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1780 - val_acc: 0.9816\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0061 - acc: 0.9989 - val_loss: 0.1802 - val_acc: 0.9814\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.0056 - acc: 0.9991 - val_loss: 0.2001 - val_acc: 0.9808\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0075 - acc: 0.9986 - val_loss: 0.1853 - val_acc: 0.9815\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0058 - acc: 0.9991 - val_loss: 0.1891 - val_acc: 0.9800\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.0050 - acc: 0.9991 - val_loss: 0.2173 - val_acc: 0.9794\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.1936 - val_acc: 0.9792\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.2195 - val_acc: 0.9775\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.0081 - acc: 0.9989 - val_loss: 0.2200 - val_acc: 0.9758\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0076 - acc: 0.9989 - val_loss: 0.1711 - val_acc: 0.9807\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2075 - val_acc: 0.9781\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.1810 - val_acc: 0.9825\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.0084 - acc: 0.9989 - val_loss: 0.1683 - val_acc: 0.9831\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0109 - acc: 0.9986 - val_loss: 0.2056 - val_acc: 0.9814\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.2237 - val_acc: 0.9794\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.2546 - val_acc: 0.9770\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0077 - acc: 0.9989 - val_loss: 0.2089 - val_acc: 0.9812\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0057 - acc: 0.9993 - val_loss: 0.1984 - val_acc: 0.9826\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.1895 - val_acc: 0.9825\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.1752 - val_acc: 0.9827\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.0067 - acc: 0.9991 - val_loss: 0.1879 - val_acc: 0.9821\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.2042 - val_acc: 0.9805\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.2076 - val_acc: 0.9809\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.2379 - val_acc: 0.9790\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.0078 - acc: 0.9990 - val_loss: 0.2218 - val_acc: 0.9803\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.0076 - acc: 0.9989 - val_loss: 0.2189 - val_acc: 0.9804\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.0088 - acc: 0.9989 - val_loss: 0.2009 - val_acc: 0.9815\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.0081 - acc: 0.9990 - val_loss: 0.2299 - val_acc: 0.9794\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.2010 - val_acc: 0.9822\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 6s 130us/step - loss: 0.0074 - acc: 0.9991 - val_loss: 0.2227 - val_acc: 0.9807\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0065 - acc: 0.9993 - val_loss: 0.1927 - val_acc: 0.9817\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.1953 - val_acc: 0.9807\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0077 - acc: 0.9989 - val_loss: 0.1833 - val_acc: 0.9817\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.2185 - val_acc: 0.9788\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 9.1599e-04 - acc: 0.9998 - val_loss: 0.2337 - val_acc: 0.9800\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 0.0080 - acc: 0.9988 - val_loss: 0.2075 - val_acc: 0.9813\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2109 - val_acc: 0.9808\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.1592 - val_acc: 0.9833\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.1719 - val_acc: 0.9833\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.0064 - acc: 0.9991 - val_loss: 0.2046 - val_acc: 0.9812\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0066 - acc: 0.9991 - val_loss: 0.1916 - val_acc: 0.9829\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.1870 - val_acc: 0.9818\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0061 - acc: 0.9993 - val_loss: 0.2331 - val_acc: 0.9804\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.0059 - acc: 0.9992 - val_loss: 0.2076 - val_acc: 0.9791\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.2245 - val_acc: 0.9789\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0078 - acc: 0.9990 - val_loss: 0.2032 - val_acc: 0.9821\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.2307 - val_acc: 0.9802\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.2222 - val_acc: 0.9804\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.0056 - acc: 0.9993 - val_loss: 0.2263 - val_acc: 0.9808\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 0.0075 - acc: 0.9993 - val_loss: 0.1998 - val_acc: 0.9820\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.0077 - acc: 0.9992 - val_loss: 0.1916 - val_acc: 0.9823\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0069 - acc: 0.9991 - val_loss: 0.1799 - val_acc: 0.9831\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0068 - acc: 0.9990 - val_loss: 0.1642 - val_acc: 0.9826\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.2129 - val_acc: 0.9809\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.1803 - val_acc: 0.9832\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.2129 - val_acc: 0.9804\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.2154 - val_acc: 0.9832\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0070 - acc: 0.9991 - val_loss: 0.2204 - val_acc: 0.9801\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0093 - acc: 0.9989 - val_loss: 0.1951 - val_acc: 0.9819\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.0059 - acc: 0.9993 - val_loss: 0.2178 - val_acc: 0.9805\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 8s 170us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.2063 - val_acc: 0.9823\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 10s 203us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.2298 - val_acc: 0.9794\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.2072 - val_acc: 0.9820\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0077 - acc: 0.9991 - val_loss: 0.2100 - val_acc: 0.9808\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 0.0068 - acc: 0.9989 - val_loss: 0.2210 - val_acc: 0.9800\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 0.1932 - val_acc: 0.9827\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 8s 163us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.2227 - val_acc: 0.9806\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.1914 - val_acc: 0.9815\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2292 - val_acc: 0.9805\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.1794 - val_acc: 0.9827\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.2084 - val_acc: 0.9814\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.0039 - acc: 0.9995 - val_loss: 0.2222 - val_acc: 0.9815\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.0078 - acc: 0.9992 - val_loss: 0.2253 - val_acc: 0.9809\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.0118 - acc: 0.9988 - val_loss: 0.2224 - val_acc: 0.9803\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.0080 - acc: 0.9992 - val_loss: 0.2199 - val_acc: 0.9809\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.1988 - val_acc: 0.9813\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 0.0089 - acc: 0.9990 - val_loss: 0.2007 - val_acc: 0.9809\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.0058 - acc: 0.9995 - val_loss: 0.2119 - val_acc: 0.9810\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.2447 - val_acc: 0.9792\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0073 - acc: 0.9991 - val_loss: 0.1896 - val_acc: 0.9839\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.2193 - val_acc: 0.9807\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.2158 - val_acc: 0.9814\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.0075 - acc: 0.9990 - val_loss: 0.2457 - val_acc: 0.9802\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0051 - acc: 0.9995 - val_loss: 0.2186 - val_acc: 0.9801\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0059 - acc: 0.9993 - val_loss: 0.2148 - val_acc: 0.9811\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.0079 - acc: 0.9992 - val_loss: 0.2485 - val_acc: 0.9800\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 0.2201 - val_acc: 0.9804\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.2206 - val_acc: 0.9810\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2203 - val_acc: 0.9817\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2079 - val_acc: 0.9831\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.0109 - acc: 0.9990 - val_loss: 0.2468 - val_acc: 0.9806\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.2199 - val_acc: 0.9819\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.0094 - acc: 0.9989 - val_loss: 0.2219 - val_acc: 0.9806\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.2448 - val_acc: 0.9799\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.0088 - acc: 0.9990 - val_loss: 0.2448 - val_acc: 0.9793\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.0056 - acc: 0.9991 - val_loss: 0.2286 - val_acc: 0.9799\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 0.0092 - acc: 0.9990 - val_loss: 0.2207 - val_acc: 0.9821\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 8s 163us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2362 - val_acc: 0.9804\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0039 - acc: 0.9995 - val_loss: 0.2179 - val_acc: 0.9807\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2451 - val_acc: 0.9812\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0083 - acc: 0.9990 - val_loss: 0.2272 - val_acc: 0.9806\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0066 - acc: 0.9993 - val_loss: 0.2238 - val_acc: 0.9816\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 0.0085 - acc: 0.9991 - val_loss: 0.2440 - val_acc: 0.9808\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0104 - acc: 0.9989 - val_loss: 0.2477 - val_acc: 0.9791\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.0102 - acc: 0.9991 - val_loss: 0.2587 - val_acc: 0.9791\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.0081 - acc: 0.9991 - val_loss: 0.2305 - val_acc: 0.9813\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.2413 - val_acc: 0.9813\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.0085 - acc: 0.9991 - val_loss: 0.2378 - val_acc: 0.9796\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.2895 - val_acc: 0.9765\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.0075 - acc: 0.9992 - val_loss: 0.2151 - val_acc: 0.9823\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.2260 - val_acc: 0.9801\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.2196 - val_acc: 0.9812\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0065 - acc: 0.9994 - val_loss: 0.2312 - val_acc: 0.9804\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.0078 - acc: 0.9990 - val_loss: 0.2316 - val_acc: 0.9803\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.2006 - val_acc: 0.9825\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2318 - val_acc: 0.9802\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.0079 - acc: 0.9992 - val_loss: 0.2314 - val_acc: 0.9813\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 0.0061 - acc: 0.9993 - val_loss: 0.2230 - val_acc: 0.9811\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.0065 - acc: 0.9993 - val_loss: 0.2009 - val_acc: 0.9831\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2528 - val_acc: 0.9799\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.0093 - acc: 0.9991 - val_loss: 0.2471 - val_acc: 0.9794\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0074 - acc: 0.9991 - val_loss: 0.2375 - val_acc: 0.9796\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0064 - acc: 0.9993 - val_loss: 0.2359 - val_acc: 0.9792\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.2142 - val_acc: 0.9815\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0047 - acc: 0.9995 - val_loss: 0.2228 - val_acc: 0.9810\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2493 - val_acc: 0.9803\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0057 - acc: 0.9993 - val_loss: 0.2446 - val_acc: 0.9801\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.0078 - acc: 0.9991 - val_loss: 0.2209 - val_acc: 0.9825\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2544 - val_acc: 0.9787\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.0063 - acc: 0.9993 - val_loss: 0.2483 - val_acc: 0.9799\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 0.0056 - acc: 0.9994 - val_loss: 0.2192 - val_acc: 0.9823\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.2431 - val_acc: 0.9817\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.0087 - acc: 0.9993 - val_loss: 0.2483 - val_acc: 0.9807\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.0071 - acc: 0.9993 - val_loss: 0.2402 - val_acc: 0.9799\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.2467 - val_acc: 0.9805\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.0052 - acc: 0.9994 - val_loss: 0.2563 - val_acc: 0.9788\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.2088 - val_acc: 0.9832\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.2427 - val_acc: 0.9814\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.2421 - val_acc: 0.9813\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2301 - val_acc: 0.9825\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0042 - acc: 0.9996 - val_loss: 0.2205 - val_acc: 0.9818\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.0046 - acc: 0.9995 - val_loss: 0.2380 - val_acc: 0.9807\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.2184 - val_acc: 0.9821\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.0061 - acc: 0.9994 - val_loss: 0.2504 - val_acc: 0.9812\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.0059 - acc: 0.9995 - val_loss: 0.2126 - val_acc: 0.9831\n"
     ]
    }
   ],
   "source": [
    "initial_net = initial_net.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=200, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_accuracy =  initial_net.history['val_acc']\n",
    "initial_loss = initial_net.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(initial_accuracy)\n",
    "plt.plot(initial_loss)\n",
    "plt.legend(['accuracy', 'loss'])\n",
    "plt.title('The validation set accuracy and loss over epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dropout = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dropout.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "network_dropout.add(layers.Dense(512, activation='relu'))\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "network_dropout.add(layers.Dense(512, activation='relu'))\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "network_dropout.add(layers.Dense(512, activation='relu'))\n",
    "network_dropout.add(layers.Dropout(0.5))\n",
    "network_dropout.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dropout.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.7252 - acc: 0.7607 - val_loss: 0.2364 - val_acc: 0.9253\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.2629 - acc: 0.9225 - val_loss: 0.1616 - val_acc: 0.9519\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.1982 - acc: 0.9429 - val_loss: 0.1196 - val_acc: 0.9662\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.1617 - acc: 0.9544 - val_loss: 0.1124 - val_acc: 0.9686\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1419 - acc: 0.9606 - val_loss: 0.0998 - val_acc: 0.9729\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.1264 - acc: 0.9646 - val_loss: 0.0902 - val_acc: 0.9759\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.1137 - acc: 0.9690 - val_loss: 0.0927 - val_acc: 0.9761\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 0.1080 - acc: 0.9696 - val_loss: 0.0873 - val_acc: 0.9758\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.1009 - acc: 0.9722 - val_loss: 0.0953 - val_acc: 0.9761\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.0920 - acc: 0.9752 - val_loss: 0.0927 - val_acc: 0.9771\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.0921 - acc: 0.9743 - val_loss: 0.0837 - val_acc: 0.9789\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0796 - acc: 0.9772 - val_loss: 0.0910 - val_acc: 0.9778\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 0.0769 - acc: 0.9785 - val_loss: 0.0911 - val_acc: 0.9791\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.0779 - acc: 0.9777 - val_loss: 0.0933 - val_acc: 0.9776\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.0735 - acc: 0.9790 - val_loss: 0.0907 - val_acc: 0.9787\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.0713 - acc: 0.9807 - val_loss: 0.0961 - val_acc: 0.9785\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 11s 223us/step - loss: 0.0694 - acc: 0.9814 - val_loss: 0.0934 - val_acc: 0.9790\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.0665 - acc: 0.9822 - val_loss: 0.1001 - val_acc: 0.9783\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 0.0647 - acc: 0.9824 - val_loss: 0.0963 - val_acc: 0.9793\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.0629 - acc: 0.9833 - val_loss: 0.0915 - val_acc: 0.9814\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.0658 - acc: 0.9824 - val_loss: 0.0944 - val_acc: 0.9797\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0616 - acc: 0.9847 - val_loss: 0.0993 - val_acc: 0.9779\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.0613 - acc: 0.9842 - val_loss: 0.0987 - val_acc: 0.9787\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.0556 - acc: 0.9854 - val_loss: 0.0961 - val_acc: 0.9811\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0532 - acc: 0.9863 - val_loss: 0.1000 - val_acc: 0.9798\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.0566 - acc: 0.9853 - val_loss: 0.0943 - val_acc: 0.9816\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.0569 - acc: 0.9858 - val_loss: 0.1068 - val_acc: 0.9786\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 0.0578 - acc: 0.9851 - val_loss: 0.1061 - val_acc: 0.9796\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.0573 - acc: 0.9852 - val_loss: 0.0960 - val_acc: 0.9813\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.0553 - acc: 0.9861 - val_loss: 0.0984 - val_acc: 0.9797\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0596 - acc: 0.9858 - val_loss: 0.0995 - val_acc: 0.9807\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0524 - acc: 0.9869 - val_loss: 0.1029 - val_acc: 0.9806\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0538 - acc: 0.9862 - val_loss: 0.1045 - val_acc: 0.9791\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.0525 - acc: 0.9866 - val_loss: 0.1113 - val_acc: 0.9803\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 0.0513 - acc: 0.9877 - val_loss: 0.1089 - val_acc: 0.9791\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 11s 226us/step - loss: 0.0562 - acc: 0.9868 - val_loss: 0.1033 - val_acc: 0.9807\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 11s 212us/step - loss: 0.0529 - acc: 0.9875 - val_loss: 0.1148 - val_acc: 0.9808\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0537 - acc: 0.9866 - val_loss: 0.1000 - val_acc: 0.9812\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 0.0525 - acc: 0.9881 - val_loss: 0.0946 - val_acc: 0.9814\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0482 - acc: 0.9886 - val_loss: 0.1042 - val_acc: 0.9807\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.0500 - acc: 0.9880 - val_loss: 0.1098 - val_acc: 0.9805\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0520 - acc: 0.9882 - val_loss: 0.1005 - val_acc: 0.9815\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0487 - acc: 0.9890 - val_loss: 0.0996 - val_acc: 0.9820\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0475 - acc: 0.9894 - val_loss: 0.1018 - val_acc: 0.9822\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0476 - acc: 0.9888 - val_loss: 0.1171 - val_acc: 0.9817\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.0502 - acc: 0.9888 - val_loss: 0.1046 - val_acc: 0.9824\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.0505 - acc: 0.9888 - val_loss: 0.1000 - val_acc: 0.9822\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0526 - acc: 0.9885 - val_loss: 0.1056 - val_acc: 0.9820\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.0455 - acc: 0.9893 - val_loss: 0.0993 - val_acc: 0.9808\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0500 - acc: 0.9885 - val_loss: 0.1106 - val_acc: 0.9808\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 0.0480 - acc: 0.9892 - val_loss: 0.1138 - val_acc: 0.9804\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0465 - acc: 0.9894 - val_loss: 0.1133 - val_acc: 0.9824\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 12s 232us/step - loss: 0.0533 - acc: 0.9892 - val_loss: 0.1093 - val_acc: 0.9806\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 0.0496 - acc: 0.9894 - val_loss: 0.1234 - val_acc: 0.9796\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 0.0506 - acc: 0.9894 - val_loss: 0.1045 - val_acc: 0.9810\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.0525 - acc: 0.9888 - val_loss: 0.1084 - val_acc: 0.9811\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 0.0482 - acc: 0.9900 - val_loss: 0.1088 - val_acc: 0.9824\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.0443 - acc: 0.9903 - val_loss: 0.1154 - val_acc: 0.9814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 0.0486 - acc: 0.9895 - val_loss: 0.1059 - val_acc: 0.9806\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0505 - acc: 0.9903 - val_loss: 0.1083 - val_acc: 0.9817\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0466 - acc: 0.9902 - val_loss: 0.1092 - val_acc: 0.9820\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.0460 - acc: 0.9906 - val_loss: 0.1227 - val_acc: 0.9816\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.0500 - acc: 0.9901 - val_loss: 0.1095 - val_acc: 0.9810\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0504 - acc: 0.9899 - val_loss: 0.1165 - val_acc: 0.9805\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 0.0471 - acc: 0.9901 - val_loss: 0.1036 - val_acc: 0.9818\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.0482 - acc: 0.9901 - val_loss: 0.1100 - val_acc: 0.9820\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.0482 - acc: 0.9909 - val_loss: 0.1145 - val_acc: 0.9810\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.0458 - acc: 0.9903 - val_loss: 0.1175 - val_acc: 0.9822\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 0.0469 - acc: 0.9901 - val_loss: 0.1171 - val_acc: 0.9813\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0483 - acc: 0.9908 - val_loss: 0.1118 - val_acc: 0.9819\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0496 - acc: 0.9903 - val_loss: 0.1286 - val_acc: 0.9799\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.0484 - acc: 0.9908 - val_loss: 0.1202 - val_acc: 0.9810\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0520 - acc: 0.9901 - val_loss: 0.1176 - val_acc: 0.9806\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0494 - acc: 0.9905 - val_loss: 0.1209 - val_acc: 0.9811\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 0.0513 - acc: 0.9906 - val_loss: 0.1308 - val_acc: 0.9811\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0495 - acc: 0.9908 - val_loss: 0.1222 - val_acc: 0.9817\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.0542 - acc: 0.9901 - val_loss: 0.1191 - val_acc: 0.9806\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.0481 - acc: 0.9913 - val_loss: 0.1214 - val_acc: 0.9811\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0472 - acc: 0.9912 - val_loss: 0.1286 - val_acc: 0.9819\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.0513 - acc: 0.9914 - val_loss: 0.1262 - val_acc: 0.9815\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 9s 178us/step - loss: 0.0531 - acc: 0.9903 - val_loss: 0.1198 - val_acc: 0.9817\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 0.0491 - acc: 0.9909 - val_loss: 0.1234 - val_acc: 0.9811\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0483 - acc: 0.9907 - val_loss: 0.1230 - val_acc: 0.9819\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0546 - acc: 0.9906 - val_loss: 0.1303 - val_acc: 0.9815\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.0559 - acc: 0.9908 - val_loss: 0.1363 - val_acc: 0.9811\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 0.0519 - acc: 0.9917 - val_loss: 0.1322 - val_acc: 0.9809\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 0.0577 - acc: 0.9906 - val_loss: 0.1213 - val_acc: 0.9817\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0539 - acc: 0.9903 - val_loss: 0.1401 - val_acc: 0.9796\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 11s 224us/step - loss: 0.0523 - acc: 0.9908 - val_loss: 0.1396 - val_acc: 0.9790\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 11s 224us/step - loss: 0.0536 - acc: 0.9906 - val_loss: 0.1288 - val_acc: 0.9813\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.0492 - acc: 0.9910 - val_loss: 0.1378 - val_acc: 0.9810\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.0562 - acc: 0.9903 - val_loss: 0.1229 - val_acc: 0.9825\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 0.0549 - acc: 0.9905 - val_loss: 0.1314 - val_acc: 0.9818\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 0.0523 - acc: 0.9909 - val_loss: 0.1270 - val_acc: 0.9824\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 9s 190us/step - loss: 0.0491 - acc: 0.9915 - val_loss: 0.1291 - val_acc: 0.9816\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.0507 - acc: 0.9915 - val_loss: 0.1300 - val_acc: 0.9810\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.0516 - acc: 0.9912 - val_loss: 0.1405 - val_acc: 0.9809\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.0525 - acc: 0.9912 - val_loss: 0.1588 - val_acc: 0.9793\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 0.0556 - acc: 0.9909 - val_loss: 0.1298 - val_acc: 0.9806\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0504 - acc: 0.9911 - val_loss: 0.1385 - val_acc: 0.9810\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0542 - acc: 0.9916 - val_loss: 0.1485 - val_acc: 0.9806\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0562 - acc: 0.9913 - val_loss: 0.1343 - val_acc: 0.9811\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.0539 - acc: 0.9917 - val_loss: 0.1561 - val_acc: 0.9816\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0499 - acc: 0.9921 - val_loss: 0.1394 - val_acc: 0.9807\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.0526 - acc: 0.9915 - val_loss: 0.1556 - val_acc: 0.9799\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0574 - acc: 0.9912 - val_loss: 0.1429 - val_acc: 0.9808\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0556 - acc: 0.9908 - val_loss: 0.1309 - val_acc: 0.9830\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.0544 - acc: 0.9914 - val_loss: 0.1406 - val_acc: 0.9822\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0523 - acc: 0.9912 - val_loss: 0.1387 - val_acc: 0.9818\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0603 - acc: 0.9905 - val_loss: 0.1394 - val_acc: 0.9803\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0501 - acc: 0.9913 - val_loss: 0.1389 - val_acc: 0.9816\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.0483 - acc: 0.9915 - val_loss: 0.1390 - val_acc: 0.9807\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.0523 - acc: 0.9915 - val_loss: 0.1386 - val_acc: 0.9807\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.0529 - acc: 0.9921 - val_loss: 0.1491 - val_acc: 0.9804\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.0533 - acc: 0.9912 - val_loss: 0.1342 - val_acc: 0.9814\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0554 - acc: 0.9922 - val_loss: 0.1299 - val_acc: 0.9812\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0543 - acc: 0.9913 - val_loss: 0.1515 - val_acc: 0.9813\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0516 - acc: 0.9919 - val_loss: 0.1508 - val_acc: 0.9805\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 0.0553 - acc: 0.9913 - val_loss: 0.1575 - val_acc: 0.9808\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.0590 - acc: 0.9916 - val_loss: 0.1421 - val_acc: 0.9816\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.0527 - acc: 0.9915 - val_loss: 0.1349 - val_acc: 0.9821\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.0603 - acc: 0.9914 - val_loss: 0.1466 - val_acc: 0.9814\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.0546 - acc: 0.9918 - val_loss: 0.1573 - val_acc: 0.9811\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.0530 - acc: 0.9922 - val_loss: 0.1485 - val_acc: 0.9830\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 11s 226us/step - loss: 0.0546 - acc: 0.9920 - val_loss: 0.1573 - val_acc: 0.9812\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.0588 - acc: 0.9914 - val_loss: 0.1613 - val_acc: 0.9808\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0564 - acc: 0.9917 - val_loss: 0.1514 - val_acc: 0.9814\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0644 - acc: 0.9913 - val_loss: 0.1617 - val_acc: 0.9805\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0547 - acc: 0.9920 - val_loss: 0.1658 - val_acc: 0.9816\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0633 - acc: 0.9910 - val_loss: 0.1647 - val_acc: 0.9801\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.0619 - acc: 0.9914 - val_loss: 0.1584 - val_acc: 0.9817\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.0664 - acc: 0.9910 - val_loss: 0.1671 - val_acc: 0.9809\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.0640 - acc: 0.9921 - val_loss: 0.1765 - val_acc: 0.9806\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 0.0601 - acc: 0.9914 - val_loss: 0.1605 - val_acc: 0.9803\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.0590 - acc: 0.9916 - val_loss: 0.1732 - val_acc: 0.9800\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 10s 197us/step - loss: 0.0650 - acc: 0.9914 - val_loss: 0.1612 - val_acc: 0.9815\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.0664 - acc: 0.9911 - val_loss: 0.1702 - val_acc: 0.9806\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 11s 210us/step - loss: 0.0627 - acc: 0.9918 - val_loss: 0.1671 - val_acc: 0.9809\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 0.0634 - acc: 0.9913 - val_loss: 0.1673 - val_acc: 0.9810\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 0.0596 - acc: 0.9921 - val_loss: 0.1727 - val_acc: 0.9798\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 9s 184us/step - loss: 0.0630 - acc: 0.9915 - val_loss: 0.1484 - val_acc: 0.9815\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.0624 - acc: 0.9920 - val_loss: 0.1729 - val_acc: 0.9807\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.0649 - acc: 0.9916 - val_loss: 0.1573 - val_acc: 0.9833\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.0591 - acc: 0.9922 - val_loss: 0.1623 - val_acc: 0.9824\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.0621 - acc: 0.9911 - val_loss: 0.1557 - val_acc: 0.9819\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.0594 - acc: 0.9916 - val_loss: 0.1551 - val_acc: 0.9814\n",
      "Epoch 147/200\n",
      "43520/50000 [=========================>....] - ETA: 1s - loss: 0.0496 - acc: 0.9930"
     ]
    }
   ],
   "source": [
    "network_dropout = network_dropout.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=200, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dropout = network_dropout.history['val_acc']\n",
    "loss_dropout = network_dropout.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_accuracy)\n",
    "plt.plot(initial_loss)\n",
    "plt.plot(accuracy_dropout)\n",
    "plt.plot(loss_dropout)\n",
    "plt.legend(['val_acc', 'val_loss', 'val_acc_dropout', 'val_loss_dropout'])\n",
    "plt.title('The validation set accuracy and loss of original model and dropout model over epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Regularisation\n",
    "\n",
    "We'll now try weight regularised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l1 = models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l1.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "network_l1.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "network_l1.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "network_l1.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "network_l1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l1 = network_l1.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=200, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l2 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l2.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l2(0.001)))\n",
    "network_l2.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "network_l2.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "network_l2.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "network_l2.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_l2 = network_l2.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=200, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_l1 = network_l1.history['val_loss']\n",
    "loss_l2 = network_l2.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(initial_loss)\n",
    "plt.plot(loss_dropout)\n",
    "plt.plot(loss_l1)\n",
    "plt.plot(loss_l2)\n",
    "plt.legend(['val_loss_org', 'val_loss_dropout', 'val_loss_l1', 'val_loss_l2'])\n",
    "plt.title('The validation loss for the initial/dropout/L1/L2 model over epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Models\n",
    "\n",
    "The first model is similar to our dropout and has one extra layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_1.add(layers.Dropout(0.5))\n",
    "model_1.add(layers.Dense(512, activation='relu'))\n",
    "model_1.add(layers.Dropout(0.5))\n",
    "model_1.add(layers.Dense(512, activation='relu'))\n",
    "model_1.add(layers.Dropout(0.5))\n",
    "model_1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second model uses one less dropout layer, and the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(512, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_2.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add more layers to our dropputs and use the ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(512, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(512, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(512, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = model_3.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a simple model with no dropouts, but use adam as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_4.add(layers.Dense(512, activation='relu'))\n",
    "model_4.add(layers.Dense(512, activation='relu'))\n",
    "model_4.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = model_4.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use an even smaller and simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_5.add(layers.Dense(512, activation='relu'))\n",
    "model_5.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_5.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = model_5.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a few more embedding layers, and use adagrad optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = model_6.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model_6.add(layers.Dense(512, activation='relu'))\n",
    "model_6.add(layers.Dense(512, activation='relu'))\n",
    "model_6.add(layers.Dense(512, activation='relu'))\n",
    "model_6.add(layers.Dense(512, activation='relu'))\n",
    "model_6.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_6 = model_6.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 7th model uses kernel regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7 = model_7.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_7.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_7.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_7.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7 = model_7.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use another regularized model, but this time with the ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_8.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_8.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8 = model_8.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model contains both the l1 and l2 regularizer, and uses a adagrad optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_9.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_9.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_9.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9 = model_9.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is a little more simple but very similar to our previous model, and uses RMSPROP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), kernel_regularizer=regularizers.l1(0.001)))\n",
    "model_10.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_10.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_10 = model_10.fit(train_images, train_labels,validation_data=(valid_images,valid_labels), epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss_drop4 = result_drop4.history['val_loss']\n",
    "# val_loss_drop3 = result_drop3.history['val_loss']\n",
    "# val_loss_drop6 = result_drop6.history['val_loss']\n",
    "# val_loss_4 = result_4.history['val_loss']\n",
    "# val_loss_3 = result_3.history['val_loss']\n",
    "# val_loss_6 = result_6.history['val_loss']\n",
    "# val_loss_l1_4 = result_l1_4.history['val_loss']\n",
    "# val_loss_l1_3 = result_l1_3.history['val_loss']\n",
    "# val_loss_l2_4 = result_l2_4.history['val_loss']\n",
    "# val_loss_l2_3 = result_l2_3.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(val_loss_dropout[:100])\n",
    "# plt.plot(val_loss_l2[:100])\n",
    "# plt.plot(val_loss_drop4)\n",
    "# plt.plot(val_loss_drop3)\n",
    "# plt.plot(val_loss_drop6)\n",
    "# plt.plot(val_loss_4)\n",
    "# plt.plot(val_loss_3)\n",
    "# plt.plot(val_loss_6)\n",
    "# plt.plot(val_loss_l1_4)\n",
    "# plt.plot(val_loss_l1_3)\n",
    "# plt.plot(val_loss_l2_4)\n",
    "# plt.plot(val_loss_l2_3)\n",
    "# plt.legend(['val_loss_dropout', 'val_loss_l2', 'val_loss_drop4', 'val_loss_drop3', 'val_loss_drop6', 'val_loss_4', 'val_loss_3', 'val_loss_6', 'val_loss_l1_4', 'val_loss_l1_3', 'val_loss_l2_4', 'val_loss_l2_3'])\n",
    "# plt.title('The validation loss for the dropout/L2 model and alternative models over epochs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInal Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
